using Test
ENV["DATADEPS_ALWAYS_ACCEPT"] = "true"
@testset "LeNet" begin
using SimpleChains, MLDatasets

lenet = SimpleChain(
  (static(28), static(28), static(1)),
  SimpleChains.Conv(SimpleChains.relu, (5, 5), 6),
  SimpleChains.MaxPool(2, 2),
  SimpleChains.Conv(SimpleChains.relu, (5, 5), 16),
  SimpleChains.MaxPool(2, 2),
  Flatten(3),
  TurboDense(SimpleChains.relu, 120),
  TurboDense(SimpleChains.relu, 84),
  TurboDense(identity, 10),
)
# 3d and 0-indexed
xtrain3, ytrain0 = MLDatasets.MNIST.traindata(Float32);
xtest3, ytest0 = MLDatasets.MNIST.testdata(Float32);
xtrain4 = reshape(xtrain3, 28, 28, 1, :);
xtest4 = reshape(xtest3, 28, 28, 1, :);
ytrain1 = UInt32.(ytrain0 .+ 1);
ytest1 = UInt32.(ytest0 .+ 1);
lenetloss = SimpleChains.add_loss(lenet, LogitCrossEntropyLoss(ytrain1));

@test SimpleChains.outputdim(lenet, size(xtrain4)) == (10, length(ytrain1));
@test SimpleChains.outputdim(lenet, size(xtest4)) == (10, length(ytest1));

# initialize parameters
@time p = SimpleChains.init_params(lenet);
@test all(isfinite, p)

@testset "Cache Corrupting Results" begin
  g = similar(p)
  subset = 1:200
  x = xtrain4[:, :, :, subset]
  y = ytrain1[subset]
  let lenetloss = SimpleChains.add_loss(lenet, SimpleChains.LogitCrossEntropyLoss(y))
    lenentmem = SimpleChains.get_heap_memory(lenetloss,0)
    lenentmem .= 0x00
    valgrad!(g, lenetloss, x, p)
    g2 = similar(g)
    lenentmem .= 0xff
    valgrad!(g2, lenetloss, x, p)
    @test g == g2
  end
end

# initialize a gradient buffer matrix; number of columns places an upper bound
# on the number of threads used.
# G = similar(p, length(p), min(Threads.nthreads(), (Sys.CPU_THREADS ÷ ((Sys.ARCH === :x86_64) + 1))));
# train
@time SimpleChains.train_batched!(p, lenetloss, xtrain4, SimpleChains.ADAM(3e-4), 10);
@test all(isfinite, p)
# @test all(isfinite, G)
# assess training and test loss
a0, l0 = SimpleChains.accuracy_and_loss(lenetloss, xtrain4, p)
a1, l1 = SimpleChains.accuracy_and_loss(lenetloss, xtest4, ytest1, p)
G = SimpleChains.alloc_threaded_grad(lenetloss);
@show size(G)
fill!(G, NaN);
# train without additional memory allocations
@time SimpleChains.train_batched!(G, p, lenetloss, xtrain4, SimpleChains.ADAM(3e-4), 10);
@test all(isfinite, p)
@test all(isfinite, G)
g = Matrix{eltype(g)}(undef, size(G,1), 1);
@time SimpleChains.train_batched!(g, p, lenetloss, xtrain4, SimpleChains.ADAM(3e-4), 2);
@test all(isfinite, p)
@test all(isfinite, g)
  
# assess training and test loss
a2, l2 = SimpleChains.accuracy_and_loss(lenetloss, xtrain4, p)
@test l2 ≈ lenetloss(xtrain4, p)
a3, l3 = SimpleChains.accuracy_and_loss(lenetloss, xtest4, ytest1, p)
@test l3 ≈ SimpleChains.add_loss(lenetloss, LogitCrossEntropyLoss(ytest1))(xtest4, p)
if size(G,2) <= 4
  @test a0 > 0.94
  @test a2 > 0.96
  @test a1 > 0.94
  @test a3 > 0.96
else
  @test a0 > 0.93
  @test a2 > 0.95
  @test a1 > 0.93
  @test a3 > 0.95
end
end

