<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · SimpleChains.jl</title><script data-outdated-warner src="assets/warner.js"></script><link rel="canonical" href="https://PumasAI.github.io/SimpleChains.jl/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>SimpleChains.jl</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="examples/smallmlp/">Small Multi-Layer Perceptron</a></li><li><a class="tocitem" href="examples/mnist/">MNIST - Convolutions</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/PumasAI/SimpleChains.jl/blob/master/docs/src/index.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="SimpleChains"><a class="docs-heading-anchor" href="#SimpleChains">SimpleChains</a><a id="SimpleChains-1"></a><a class="docs-heading-anchor-permalink" href="#SimpleChains" title="Permalink"></a></h1><p>Documentation for <a href="https://github.com/PumasAI/SimpleChains.jl">SimpleChains</a>.</p><ul><li><a href="#SimpleChains.ADAM"><code>SimpleChains.ADAM</code></a></li><li><a href="#SimpleChains.AbsoluteLoss"><code>SimpleChains.AbsoluteLoss</code></a></li><li><a href="#SimpleChains.AbstractPenalty"><code>SimpleChains.AbstractPenalty</code></a></li><li><a href="#SimpleChains.Activation"><code>SimpleChains.Activation</code></a></li><li><a href="#SimpleChains.Conv"><code>SimpleChains.Conv</code></a></li><li><a href="#SimpleChains.Dropout"><code>SimpleChains.Dropout</code></a></li><li><a href="#SimpleChains.Flatten"><code>SimpleChains.Flatten</code></a></li><li><a href="#SimpleChains.FrontLastPenalty"><code>SimpleChains.FrontLastPenalty</code></a></li><li><a href="#SimpleChains.L1Penalty"><code>SimpleChains.L1Penalty</code></a></li><li><a href="#SimpleChains.L2Penalty"><code>SimpleChains.L2Penalty</code></a></li><li><a href="#SimpleChains.LogitCrossEntropyLoss"><code>SimpleChains.LogitCrossEntropyLoss</code></a></li><li><a href="#SimpleChains.MaxPool"><code>SimpleChains.MaxPool</code></a></li><li><a href="#SimpleChains.SimpleChain"><code>SimpleChains.SimpleChain</code></a></li><li><a href="#SimpleChains.SquaredLoss"><code>SimpleChains.SquaredLoss</code></a></li><li><a href="#SimpleChains.TurboDense"><code>SimpleChains.TurboDense</code></a></li><li><a href="#Base.front-Tuple{SimpleChain}"><code>Base.front</code></a></li><li><a href="#SimpleChains.add_loss-Tuple{SimpleChain, SimpleChains.AbstractLoss}"><code>SimpleChains.add_loss</code></a></li><li><a href="#SimpleChains.alloc_threaded_grad-Union{Tuple{SimpleChain}, Tuple{T}, Tuple{SimpleChain, Union{Nothing, SimpleChains.InputDimUnknown, Tuple{Vararg{Integer}}}}, Tuple{SimpleChain, Union{Nothing, SimpleChains.InputDimUnknown, Tuple{Vararg{Integer}}}, Type{T}}} where T"><code>SimpleChains.alloc_threaded_grad</code></a></li><li><a href="#SimpleChains.init_params-Union{Tuple{T}, Tuple{SimpleChain, Type{T}}} where T"><code>SimpleChains.init_params</code></a></li><li><a href="#SimpleChains.init_params!"><code>SimpleChains.init_params!</code></a></li><li><a href="#SimpleChains.numparam-Tuple{TurboDense, Tuple}"><code>SimpleChains.numparam</code></a></li><li><a href="#SimpleChains.params"><code>SimpleChains.params</code></a></li><li><a href="#SimpleChains.train_batched!-Tuple{AbstractVecOrMat, AbstractVector, Union{SimpleChains.AbstractPenalty{&lt;:SimpleChain}, SimpleChain}, Any, SimpleChains.AbstractOptimizer, Any}"><code>SimpleChains.train_batched!</code></a></li><li><a href="#SimpleChains.train_unbatched!-Tuple{Any, Any, Union{SimpleChains.AbstractPenalty{&lt;:SimpleChain}, SimpleChain}, Any, SimpleChains.AbstractOptimizer, Int64}"><code>SimpleChains.train_unbatched!</code></a></li><li><a href="#SimpleChains.valgrad!-Tuple{Any, SimpleChain, Any, Any}"><code>SimpleChains.valgrad!</code></a></li></ul><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.ADAM" href="#SimpleChains.ADAM"><code>SimpleChains.ADAM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ADAM(η = 0.001, β = (0.9, 0.999))</code></pre><p>ADAM optimizer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/optimize.jl#L4-L8">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.AbsoluteLoss" href="#SimpleChains.AbsoluteLoss"><code>SimpleChains.AbsoluteLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbsoluteLoss</code></pre><p>Calculates mean absolute loss of the target.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/loss.jl#L114-L118">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.AbstractPenalty" href="#SimpleChains.AbstractPenalty"><code>SimpleChains.AbstractPenalty</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractPenalty</code></pre><p>The <code>AbstractPenalty</code> interface requires supporting the following methods:</p><ol><li><code>getchain(::AbstractPenalty)::SimpleChain</code> returns a <code>SimpleChain</code> if it is carrying one.</li><li><code>apply_penalty(::AbstractPenalty, params)::Number</code> returns the penalty</li><li><code>apply_penalty!(grad, ::AbstractPenalty, params)::Number</code> returns the penalty and updates <code>grad</code> to add the gradient.</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/penalty.jl#L2-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.Activation" href="#SimpleChains.Activation"><code>SimpleChains.Activation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Activation(activation)</code></pre><p>Applies <code>activation</code> function elementwise.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/activation.jl#L4-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.Conv" href="#SimpleChains.Conv"><code>SimpleChains.Conv</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Conv(activation, dims::Tuple{Vararg{Integer}}, outputdim::Integer)</code></pre><p>Performs a convolution with <code>dims</code> and maps it to <code>outputdim</code> output channels, then adds a bias (one per <code>outputdim</code>) and applies <code>activation</code> elementwise.</p><p>E.g., <code>Conv(relu, (5, 5), 16)</code> performs a <code>5 × 5</code> convolution, and maps the input channels to 16 output channels, before adding a bias and applying <code>relu</code>.</p><p>Randomly initializing weights using the (Xavier) Glorot uniform distribution. The bias is zero-initialized.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/conv.jl#L690-L701">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.Dropout" href="#SimpleChains.Dropout"><code>SimpleChains.Dropout</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Dropout(p) # 0 &lt; p &lt; 1</code></pre><p>Dropout layer.</p><p>When evaluated without gradients, it multiplies inputs by <code>(1 - p)</code>. When evaluated with gradients, it randomly zeros <code>p</code> proportion of inputs.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/dropout.jl#L3-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.Flatten" href="#SimpleChains.Flatten"><code>SimpleChains.Flatten</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Flatten{N}()</code></pre><p>Flattens the first <code>N</code> dimensions. E.g.,</p><pre><code class="language-julia hljs">julia&gt; Flatten{2}()(rand(2,3,4))
6×4 Matrix{Float64}:
 0.0609115  0.597285  0.279899  0.888223
 0.0667422  0.315741  0.351003  0.805629
 0.678297   0.350817  0.984215  0.399418
 0.125801   0.566696  0.96873   0.57744
 0.331961   0.350742  0.59598   0.741998
 0.26345    0.144635  0.076433  0.330475</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/flatten.jl#L2-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.FrontLastPenalty" href="#SimpleChains.FrontLastPenalty"><code>SimpleChains.FrontLastPenalty</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FrontLastPenalty(SimpleChain, frontpen(λ₁...), lastpen(λ₂...))</code></pre><p>Applies <code>frontpen</code> to all but the last layer, applying <code>lastpen</code> to the last layer instead. &quot;Last layer&quot; here ignores the loss function, i.e. if the last element of the chain is a loss layer, the then <code>lastpen</code> applies to the layer preceding this.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/penalty.jl#L157-L163">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.L1Penalty" href="#SimpleChains.L1Penalty"><code>SimpleChains.L1Penalty</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L1Penalty(λ)</code></pre><p>Applies a L1 penalty of <code>λ</code> to parameters, i.e. penalizing by their absolute value.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/penalty.jl#L77-L81">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.L2Penalty" href="#SimpleChains.L2Penalty"><code>SimpleChains.L2Penalty</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L2Penalty(λ)</code></pre><p>Applies a L2 penalty of <code>λ</code> to parameters, i.e. penalizing by their squares.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/penalty.jl#L117-L121">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.LogitCrossEntropyLoss" href="#SimpleChains.LogitCrossEntropyLoss"><code>SimpleChains.LogitCrossEntropyLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LogitCrossEntropyLoss</code></pre><p>Calculates mean logit cross-entropy loss.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/loss.jl#L169-L173">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.MaxPool" href="#SimpleChains.MaxPool"><code>SimpleChains.MaxPool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MaxPool(dims::Tuple{Vararg{Integer}}</code></pre><p>Calculates the maximum of pools of size <code>dims</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/maxpool.jl#L2-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.SimpleChain" href="#SimpleChains.SimpleChain"><code>SimpleChains.SimpleChain</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SimpleChain([inputdim::Union{Integer,Tuple{Vararg{Integer}}, ] layers)</code></pre><p>Construct a SimpleChain. Optional <code>inputdim</code> argument allows <code>SimpleChains</code> to check the size of inputs. Making these <code>static</code> will allow <code>SimpleChains</code> to infer size and loop bounds at compile time. Batch size generally should not be included in the <code>inputdim</code>. If <code>inputdim</code> is not specified, some methods, e.g. <code>init_params</code>, will require passing the size as an additional argument, because the number of parameters may be a function of the input size (e.g., for a <code>TurboDense</code> layer).</p><p>The <code>layers</code> argument holds various <code>SimpleChains</code> layers, e.g. <code>TurboDense</code>, <code>Conv</code>, <code>Activation</code>, <code>Flatten</code>, <code>Dropout</code>, or <code>MaxPool</code>. It may optionally terminate in an <code>AbstractLoss</code> layer.</p><p>These objects are callable, e.g.</p><pre><code class="language-julia hljs">c = SimpleChain(...);
p = SimpleChains.init_params(c);
c(X, p) # X are the independent variables, and `p` the parameter vector.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/simple_chain.jl#L5-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.SquaredLoss" href="#SimpleChains.SquaredLoss"><code>SimpleChains.SquaredLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SquaredLoss(target)</code></pre><p>Calculates half of mean squared loss of the target.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/loss.jl#L58-L62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.TurboDense" href="#SimpleChains.TurboDense"><code>SimpleChains.TurboDense</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TurboDense{B=true}(activation, outputdim::Integer)</code></pre><p>Linear (dense) layer.</p><ul><li><code>B</code> specifies whether the layer includes a bias term.</li><li>The <code>activation</code> function is applied elementwise to the result.</li><li><code>outputdim</code> indicates how many dimensions the input is mapped to.</li></ul><p>Randomly initializing weights using the (Xavier) Glorot normal distribution. The bias is zero-initialized.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/dense.jl#L2-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.front-Tuple{SimpleChain}" href="#Base.front-Tuple{SimpleChain}"><code>Base.front</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">Base.front(c::SimpleChain)</code></pre><p>Useful for popping off a loss layer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/simple_chain.jl#L93-L97">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.add_loss-Tuple{SimpleChain, SimpleChains.AbstractLoss}" href="#SimpleChains.add_loss-Tuple{SimpleChain, SimpleChains.AbstractLoss}"><code>SimpleChains.add_loss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">add_loss(chn, l::AbstractLoss)</code></pre><p>Add the loss function <code>l</code> to the simple chain. The loss function should hold the target you&#39;re trying to fit.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/loss.jl#L4-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.alloc_threaded_grad-Union{Tuple{SimpleChain}, Tuple{T}, Tuple{SimpleChain, Union{Nothing, SimpleChains.InputDimUnknown, Tuple{Vararg{Integer}}}}, Tuple{SimpleChain, Union{Nothing, SimpleChains.InputDimUnknown, Tuple{Vararg{Integer}}}, Type{T}}} where T" href="#SimpleChains.alloc_threaded_grad-Union{Tuple{SimpleChain}, Tuple{T}, Tuple{SimpleChain, Union{Nothing, SimpleChains.InputDimUnknown, Tuple{Vararg{Integer}}}}, Tuple{SimpleChain, Union{Nothing, SimpleChains.InputDimUnknown, Tuple{Vararg{Integer}}}, Type{T}}} where T"><code>SimpleChains.alloc_threaded_grad</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">alloc_threaded_grad(chn, id = nothing, ::Type{T} = Float32; numthreads = min(Threads.nthreads(), SimpleChains.num_cores())</code></pre><p>Returns a preallocated array for writing gradients, for use with <code>train_batched</code> and <code>train_unbatched</code>. If Julia was started with multiple threads, returns a matrix with one column per thread, so they may accumulate gradients in parallel.</p><p>Note that the memory is alligned to avoid false sharing.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/utils.jl#L198-L206">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.init_params!" href="#SimpleChains.init_params!"><code>SimpleChains.init_params!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">SimpleChains.init_params!(chn, p, id = nothing)</code></pre><p>Randomly initializes parameter vector <code>p</code> with input dim <code>id</code>. Input dim does not need to be specified if these were provided to the chain object itself. See the documentation of the individual layers to see how they are initialized, but it is generally via (Xavier) Glorot uniform or normal distributions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/simple_chain.jl#L251-L256">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.init_params-Union{Tuple{T}, Tuple{SimpleChain, Type{T}}} where T" href="#SimpleChains.init_params-Union{Tuple{T}, Tuple{SimpleChain, Type{T}}} where T"><code>SimpleChains.init_params</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">SimpleChains.init_params(chn[, id = nothing][, ::Type{T} = Float32])</code></pre><p>Creates a parameter vector of element type <code>T</code> with size matching that by <code>id</code> (argument not required if provided to the <code>chain</code> object itself). See the documentation of the individual layers to see how they are initialized, but it is generally via (Xavier) Glorot uniform or normal distributions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/simple_chain.jl#L274-L279">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.numparam-Tuple{TurboDense, Tuple}" href="#SimpleChains.numparam-Tuple{TurboDense, Tuple}"><code>SimpleChains.numparam</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">numparam(d::Layer, inputdim::Tuple)</code></pre><p>Returns a <code>Tuple{Int,S}</code>. The first element is the number of parameters required by the layer given an argument of size <code>inputdim</code>. The second argument is the size of the object returned by the layer, which can be fed into <code>numparam</code> of the following layer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/dense.jl#L41-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.params" href="#SimpleChains.params"><code>SimpleChains.params</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">params(sc::SimpleChain, p::AbstractVector, inputdim = nothing)</code></pre><p>Returns a tuple of the parameters of the SimpleChain <code>sc</code>, as a view of the parameter vector <code>p</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/utils.jl#L229-L233">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.train_batched!-Tuple{AbstractVecOrMat, AbstractVector, Union{SimpleChains.AbstractPenalty{&lt;:SimpleChain}, SimpleChain}, Any, SimpleChains.AbstractOptimizer, Any}" href="#SimpleChains.train_batched!-Tuple{AbstractVecOrMat, AbstractVector, Union{SimpleChains.AbstractPenalty{&lt;:SimpleChain}, SimpleChain}, Any, SimpleChains.AbstractOptimizer, Any}"><code>SimpleChains.train_batched!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">train_batched!(g::AbstractVecOrMat, p, chn, X, opt, iters; batchsize = nothing)</code></pre><p>Train while batching arguments.</p><p>Arguments:</p><ul><li><code>g</code> pre-allocated gradient buffer. Can be allocated with <code>similar(p)</code> (if you want to run single threaded), or <code>alloc_threaded_grad(chn, size(X))</code> (<code>size(X)</code> argument is only necessary if the input dimension was not specified when constructing the chain). If a matrix, the number of columns gives how many threads to use. Do not use more threads than batch size would allow.</li><li><code>p</code> is the parameter vector. It is updated inplace. It should be pre-initialized, e.g. with <code>init_params</code>/<code>init_params!</code>. This is to allow calling <code>train_unbatched!</code> several times to train in increments.</li><li><code>chn</code> is the <code>SimpleChain</code>. It must include a loss (see <code>SimpleChains.add_loss</code>) containing the target information (dependent variables) you&#39;re trying to fit.</li><li><code>X</code> the training data input argument (independent variables).</li><li><code>opt</code> is the optimizer. Currently, only <code>SimpleChains.ADAM</code> is supported.</li><li><code>iters</code>, how many iterations to train for.</li><li><code>batchsize</code> keyword argument: the size of the batches to use. If <code>batchsize = nothing</code>, it&#39;ll try to do a half-decent job of picking the batch size for you. However, this is not well optimized at the moment.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/optimize.jl#L407-L420">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.train_unbatched!-Tuple{Any, Any, Union{SimpleChains.AbstractPenalty{&lt;:SimpleChain}, SimpleChain}, Any, SimpleChains.AbstractOptimizer, Int64}" href="#SimpleChains.train_unbatched!-Tuple{Any, Any, Union{SimpleChains.AbstractPenalty{&lt;:SimpleChain}, SimpleChain}, Any, SimpleChains.AbstractOptimizer, Int64}"><code>SimpleChains.train_unbatched!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">train_unbatched!(g::AbstractVecOrMat, p, chn, X, opt, iters)</code></pre><p>Train without batching inputs.</p><p>Arguments:</p><ul><li><code>g</code> pre-allocated gradient buffer. Can be allocated with <code>similar(p)</code> (if you want to run single threaded), or <code>alloc_threaded_grad(chn, size(X))</code> (<code>size(X)</code> argument is only necessary if the input dimension was not specified when constructing the chain). If a matrix, the number of columns gives how many threads to use. Do not use more threads than batch size would allow.</li><li><code>p</code> is the parameter vector. It is updated inplace. It should be pre-initialized, e.g. with <code>init_params</code>/<code>init_params!</code>. This is to allow calling <code>train_unbatched!</code> several times to train in increments.</li><li><code>chn</code> is the <code>SimpleChain</code>. It must include a loss (see <code>SimpleChains.add_loss</code>) containing the target information (dependent variables) you&#39;re trying to fit.</li><li><code>X</code> the training data input argument (independent variables).</li><li><code>opt</code> is the optimizer. Currently, only <code>SimpleChains.ADAM</code> is supported.</li><li><code>iters</code>, how many iterations to train for.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/optimize.jl#L315-L327">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleChains.valgrad!-Tuple{Any, SimpleChain, Any, Any}" href="#SimpleChains.valgrad!-Tuple{Any, SimpleChain, Any, Any}"><code>SimpleChains.valgrad!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Allowed destruction:</p><pre><code class="nohighlight hljs">valgrad_layer!</code></pre><p>Accepts return of previous layer (<code>B</code>) and returns an ouput <code>C</code>. If an internal layer, allowed to destroy <code>B</code> (e.g. dropout layer).</p><pre><code class="nohighlight hljs">pullback!</code></pre><p>Accepts adjoint of its return (<code>C̄</code>). It is allowed to destroy this. It is also allowed to destroy the previous layer&#39;s return <code>B</code> to produce <code>B̄</code> (the <code>C̄</code> it receives). Thus, the pullback is not allowed to depend on <code>C</code>, as it may have been destroyed in producing <code>C̄</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PumasAI/SimpleChains.jl/blob/3ca30d387c3d6c43094d177cfdb9427e716ef665/src/simple_chain.jl#L289-L300">source</a></section></article><ul><li><a href="examples/smallmlp/#Small-Multi-Layer-Perceptron">Small Multi-Layer Perceptron</a></li><li><a href="examples/mnist/#MNIST-Convolutions">MNIST - Convolutions</a></li></ul></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="examples/smallmlp/">Small Multi-Layer Perceptron »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.17 on <span class="colophon-date" title="Sunday 22 May 2022 17:00">Sunday 22 May 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
